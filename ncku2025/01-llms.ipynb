{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d2af97-28f5-46c2-a1a9-5589c26371e9",
   "metadata": {},
   "source": [
    "# LLMs\n",
    "- https://python.langchain.com/docs/introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115a829-202a-4c7e-8605-4eb4bb9b0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm模型設定\n",
    "# https://build.nvidia.com/deepseek-ai/deepseek-r1\n",
    "# nvapi-xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "  os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n",
    "\n",
    "## 模型來源\n",
    "# https://build.nvidia.com/deepseek-ai\n",
    "# https://build.nvidia.com/meta\n",
    "# https://build.nvidia.com/google\n",
    "# https://build.nvidia.com/qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda8606-80c8-4156-a288-61c3f774e1a9",
   "metadata": {},
   "source": [
    "# OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffed76e-5893-470c-a7e7-711aad9275d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -qU \"openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b2a7f-2e42-41a9-9e2a-2497ea18d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Using General OpenAI Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI()  ## Assumes OPENAI_API_KEY is set\n",
    "client = OpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta/llama-4-maverick-17b-128e-instruct\",\n",
    "    # model=\"gpt-4-turbo-2024-04-09\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"Translate the following from English into Italian\"},\n",
    "        {\"role\":\"user\",\"content\":\"Hi\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "## Streaming with Generator: Results come out as they're generated\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c7473-f45a-435e-81c3-1ff06f1370bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-Streaming: Results come from server when they're all ready\n",
    "##  Using General OpenAI Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI()  ## Assumes OPENAI_API_KEY is set\n",
    "client = OpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta/llama-4-maverick-17b-128e-instruct\",\n",
    "    # model=\"gpt-4-turbo-2024-04-09\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"Translate the following from English into Italian\"},\n",
    "        {\"role\":\"user\",\"content\":\"Hi\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "#completion\n",
    "if completion.choices and completion.choices[0].message:\n",
    "    print(completion.choices[0].message.content)\n",
    "else:\n",
    "    print(\"No response content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0928bb-f249-4d1c-b731-24402c74374b",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d380c-8770-4b24-9451-a44054daa9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -qU \"langchain-nvidia-ai-endpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ffd78-6fb6-4c89-9d9d-eb29e3f1eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"meta/llama-4-maverick-17b-128e-instruct\", model_provider=\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba41d31-d761-4695-99c7-279749b7860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "response=llm.invoke(messages)\n",
    "print(response)\n",
    "print(\"---------------\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5385f97-0d69-4547-830d-026a70e449b5",
   "metadata": {},
   "source": [
    "## 作業\n",
    "**作業：**\n",
    "\n",
    "1.  **模型選擇：**\n",
    "    * 選擇兩個不同的 LLM 模型來完成以下任務。\n",
    "    * 請勿使用 notebook 中已使用的 `meta/llama-4-maverick-17b-128e-instruct` 模型。\n",
    "\n",
    "2.  **聊天任務：**\n",
    "    * 設計一個模擬客戶服務諮詢的多輪對話，目標是解決客戶關於產品退貨的問題。\n",
    "    * 對話至少包含 5 輪，並包含以下元素：\n",
    "        * 客戶提出退貨原因\n",
    "        * 客服詢問詳細資訊 (例如：訂單編號、產品狀況)\n",
    "        * 客服說明退貨流程與注意事項\n",
    "        * 客戶追問相關問題\n",
    "        * 客服總結並提供協助\n",
    "\n",
    "3.  **模型比較與分析：**\n",
    "    * 使用選擇的兩個模型分別執行上述多輪對話。\n",
    "    * 比較兩個模型的輸出，並分析其在以下方面的差異：\n",
    "        * **準確性：** 是否正確理解客戶問題並提供正確資訊？\n",
    "        * **流暢度：** 語言是否自然、符合客服人員的說話方式？\n",
    "        * **相關性：** 回答是否切題、符合對話上下文？\n",
    "        * **效率：** 回應速度如何？\n",
    "    * 使用表格或簡短報告總結比較結果，並說明哪個模型更適合用於客戶服務情境，以及原因。\n",
    "\n",
    "**評分標準：**\n",
    "\n",
    "* 模型選擇與設定 (20%)\n",
    "* 多輪對話設計 (30%)\n",
    "* 模型輸出與比較 (30%)\n",
    "* 分析與結論 (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c03502-1828-4d8e-80de-438492f15715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
