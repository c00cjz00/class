{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c7ba09-bd42-4147-b1a5-9b91a1cf2be9",
   "metadata": {},
   "source": [
    "# Embedding models\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace885b-a606-44fa-9dfa-e263937d4438",
   "metadata": {},
   "source": [
    "## 比較各家 embedding模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5ce0b-1de5-4f03-b4b6-e6fee10f0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa07228-4609-4b9a-b82c-45ac759d0c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for NVIDIA:  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\uv\\langchaing\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:212: UserWarning: Found meta/llama-4-maverick-17b-128e-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# embedding模型設定\n",
    "# https://build.nvidia.com/nvidia/nv-embed-v1\n",
    "# nvapi-xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "  os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"meta/llama-4-maverick-17b-128e-instruct\", model_provider=\"nvidia\")\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "embeddings = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e4a8f-3bd2-469e-b726-eda0f7511884",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42d735-2c69-4817-963b-18000906eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-voyageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442efee-980b-430f-af8b-0d29235adc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding模型設定\n",
    "# https://www.voyageai.com/\n",
    "# https://dashboard.voyageai.com/api-keys\n",
    "# pa-\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"VOYAGE_API_KEY\"):\n",
    "  os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n",
    "    \n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "embeddings = VoyageAIEmbeddings(model=\"voyage-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd1de2-9d4e-4c08-b304-30c17e0bd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0ab22-50f6-420d-8df6-19cea423aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f1215-9dc6-4ced-8b89-8e9bdb036cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding模型設定\n",
    "# https://jina.ai/\n",
    "# jina_xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"JINA_API_KEY\"):\n",
    "  os.environ[\"JINA_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n",
    "\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "embeddings = JinaEmbeddings(\n",
    "    jina_api_key=os.environ[\"JINA_API_KEY\"], model_name=\"jina-embeddings-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194cfb8-622a-4977-8fec-da4d18f424f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff71da-78ab-4c80-bea2-71a1c9e32a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccf0d72-fa37-49dc-973e-fcc06ac00ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Voyage AI:  ········\n"
     ]
    }
   ],
   "source": [
    "# embedding模型設定\n",
    "# https://jina.ai/\n",
    "# jina_xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"JINA_API_KEY\"):\n",
    "  os.environ[\"JINA_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n",
    "\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "embeddings = JinaEmbeddings(\n",
    "    jina_api_key=os.environ[\"JINA_API_KEY\"], model_name=\"jina-embeddings-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f29d06f-ca79-496e-a6a6-c897cf84ec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08977351, -0.079159915, 0.009447592, 0.012913363, 0.059213284]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    \"Caching embeddings enables the storage or temporary caching of embeddings, eliminating the necessity to recompute them each time.\",\n",
    "    \"An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\",\n",
    "    \"A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.\",\n",
    "]\n",
    "documents_embds = embeddings.embed_documents(documents)\n",
    "documents_embds[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a835850-a3ec-43f9-84da-12b48345dd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11538703, -0.03468972, 0.030259516, -0.010401461, 0.07359464]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's an LLMChain?\"\n",
    "query_embd = embeddings.embed_query(query)\n",
    "query_embd[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e141b-e563-437a-987e-dbc5615bc2e6",
   "metadata": {},
   "source": [
    "## KNNRetriever\n",
    "- https://python.langchain.com/docs/integrations/retrievers\n",
    "- https://python.langchain.com/docs/integrations/retrievers/knn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6216988-6f6f-451c-b490-90a0a6ec3e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import KNNRetriever\n",
    "\n",
    "documents = [\n",
    "    \"Caching embeddings enables the storage or temporary caching of embeddings, eliminating the necessity to recompute them each time.\",\n",
    "    \"An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\",\n",
    "    \"A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.\",\n",
    "]\n",
    "\n",
    "query = \"What's an LLMChain?\"\n",
    "\n",
    "retriever = KNNRetriever.from_texts(documents, embeddings,k=2)\n",
    "\n",
    "# retrieve the most relevant documents\n",
    "result = retriever.invoke(query)\n",
    "top1_retrieved_doc = result[0].page_content  # return the top1 retrieved result\n",
    "\n",
    "print(top1_retrieved_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57bc58c-c630-4ea4-926d-2d247a47a08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What's an LLMChain?\",\n",
       " 'context': [Document(metadata={}, page_content='An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.'),\n",
       "  Document(metadata={}, page_content='A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.')],\n",
       " 'answer': 'An LLMChain is a chain that composes basic LLM functionality, consisting of a PromptTemplate and a language model. It formats the prompt using input and memory key values, and returns the LLM output. It combines these elements to facilitate language model interactions.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "#retriever = KNNRetriever.from_texts(documents, embeddings,k=2)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "query = \"What's an LLMChain?\"\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ae88e-b515-436c-9912-e1d92dcded15",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90b3044-37a4-48b6-b2c9-d644fc368e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -qU  rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf362f2c-d20b-4ba9-8f17-2efd66f27b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.'),\n",
       " Document(metadata={}, page_content='A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "retriever = BM25Retriever.from_texts(documents,k=2)\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2e2e36-c412-4d9c-89c4-e7759099707c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What's an LLMChain?\",\n",
       " 'context': [Document(metadata={}, page_content='An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.'),\n",
       "  Document(metadata={}, page_content='A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.')],\n",
       " 'answer': 'An LLMChain is a chain that composes basic LLM functionality, consisting of a PromptTemplate and a language model. It formats the prompt template using input key values and memory key values, then passes the formatted string to the LLM. It returns the LLM output.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "#retriever = KNNRetriever.from_texts(documents, embeddings,k=2)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "query = \"What's an LLMChain?\"\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5082f-e746-4d2b-88ca-490d52e9a5ee",
   "metadata": {},
   "source": [
    "##   🧠 作業三：語意理解任務中的 Embedding 實驗設計\n",
    "\n",
    "**目標：**\n",
    "\n",
    "* 分析 embedding 模型在不同語境下的表現\n",
    "* 比較語意相近但字面不同的例子\n",
    "* 比較不同 embedding 模型在相同任務上的表現 (可選擇)\n",
    "\n",
    "**內容：**\n",
    "\n",
    "1.  **設計句子對：**\n",
    "    * 設計十組句子對，每組包含以下至少兩種語意關係的組合：\n",
    "        * **同義：** 「他很開心」 vs 「他感到愉快」\n",
    "        * **近義：** 「這輛車很新」 vs 「這輛車是全新的」\n",
    "        * **反義：** 「他很高」 vs 「他很矮」\n",
    "        * **上位/下位詞：** 「狗是一種動物」 vs 「黃金獵犬是一種狗」\n",
    "        * **因果關係：** 「因為下雨，所以路很濕」 vs 「路很濕是因為下雨」\n",
    "        * (鼓勵加入複雜句或多義詞)\n",
    "    * 確保每組句子對中，至少有一對是語意相近，另一對是語意相異。\n",
    "\n",
    "2.  **計算相似度：**\n",
    "    * 選擇一個或多個 embedding 模型 (例如： notebook 中提到的或其他你感興趣的模型)。\n",
    "    * 使用選擇的 embedding 模型計算每組句子對之間的相似度。\n",
    "    * 建議使用餘弦相似度 (Cosine Similarity) 作為計算方法。\n",
    "\n",
    "3.  **分析與評估：**\n",
    "    * 分析模型的語意理解能力：\n",
    "        * 模型是否能有效分辨語意相近與語意相異的句子對？\n",
    "        * 模型對於不同語意關係的處理能力是否有差異？\n",
    "        * 模型在處理複雜句或多義詞時的表現如何？\n",
    "    * 使用以下指標評估模型表現：\n",
    "        * **準確度：** 模型是否能正確區分語意相近和語意相異的句子對？ (可自行定義判斷標準)\n",
    "        * **排序能力：** 如果將所有句子對依相似度排序，語意更相近的句子對是否排在更前面？\n",
    "    * (如果選擇多個 embedding 模型) 比較不同模型之間的差異。\n",
    "\n",
    "**評分標準：**\n",
    "\n",
    "* 句子對設計的完整性與多樣性 (40%)\n",
    "* 相似度計算的正確性 (30%)\n",
    "* 分析與評估的深度 (30%)\n",
    "* 報告的清晰度與完整性 (20%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0442c-774a-4cb6-93fa-f8bff74e0a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
