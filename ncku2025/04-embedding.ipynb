{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c7ba09-bd42-4147-b1a5-9b91a1cf2be9",
   "metadata": {},
   "source": [
    "# Embedding models\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace885b-a606-44fa-9dfa-e263937d4438",
   "metadata": {},
   "source": [
    "## æ¯”è¼ƒå„å®¶ embeddingæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5ce0b-1de5-4f03-b4b6-e6fee10f0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa07228-4609-4b9a-b82c-45ac759d0c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for NVIDIA:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\uv\\langchaing\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:212: UserWarning: Found meta/llama-4-maverick-17b-128e-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# embeddingæ¨¡å‹è¨­å®š\n",
    "# https://build.nvidia.com/nvidia/nv-embed-v1\n",
    "# nvapi-xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "  os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"meta/llama-4-maverick-17b-128e-instruct\", model_provider=\"nvidia\")\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "embeddings = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e4a8f-3bd2-469e-b726-eda0f7511884",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42d735-2c69-4817-963b-18000906eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-voyageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442efee-980b-430f-af8b-0d29235adc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddingæ¨¡å‹è¨­å®š\n",
    "# https://www.voyageai.com/\n",
    "# https://dashboard.voyageai.com/api-keys\n",
    "# pa-\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"VOYAGE_API_KEY\"):\n",
    "  os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n",
    "    \n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "embeddings = VoyageAIEmbeddings(model=\"voyage-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd1de2-9d4e-4c08-b304-30c17e0bd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0ab22-50f6-420d-8df6-19cea423aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f1215-9dc6-4ced-8b89-8e9bdb036cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddingæ¨¡å‹è¨­å®š\n",
    "# https://jina.ai/\n",
    "# jina_xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"JINA_API_KEY\"):\n",
    "  os.environ[\"JINA_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n",
    "\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "embeddings = JinaEmbeddings(\n",
    "    jina_api_key=os.environ[\"JINA_API_KEY\"], model_name=\"jina-embeddings-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194cfb8-622a-4977-8fec-da4d18f424f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff71da-78ab-4c80-bea2-71a1c9e32a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccf0d72-fa37-49dc-973e-fcc06ac00ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Voyage AI:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "# embeddingæ¨¡å‹è¨­å®š\n",
    "# https://jina.ai/\n",
    "# jina_xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"JINA_API_KEY\"):\n",
    "  os.environ[\"JINA_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n",
    "\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "embeddings = JinaEmbeddings(\n",
    "    jina_api_key=os.environ[\"JINA_API_KEY\"], model_name=\"jina-embeddings-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f29d06f-ca79-496e-a6a6-c897cf84ec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08977351, -0.079159915, 0.009447592, 0.012913363, 0.059213284]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    \"Caching embeddings enables the storage or temporary caching of embeddings, eliminating the necessity to recompute them each time.\",\n",
    "    \"An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\",\n",
    "    \"A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.\",\n",
    "]\n",
    "documents_embds = embeddings.embed_documents(documents)\n",
    "documents_embds[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a835850-a3ec-43f9-84da-12b48345dd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11538703, -0.03468972, 0.030259516, -0.010401461, 0.07359464]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's an LLMChain?\"\n",
    "query_embd = embeddings.embed_query(query)\n",
    "query_embd[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e141b-e563-437a-987e-dbc5615bc2e6",
   "metadata": {},
   "source": [
    "## KNNRetriever\n",
    "- https://python.langchain.com/docs/integrations/retrievers\n",
    "- https://python.langchain.com/docs/integrations/retrievers/knn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6216988-6f6f-451c-b490-90a0a6ec3e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import KNNRetriever\n",
    "\n",
    "documents = [\n",
    "    \"Caching embeddings enables the storage or temporary caching of embeddings, eliminating the necessity to recompute them each time.\",\n",
    "    \"An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\",\n",
    "    \"A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.\",\n",
    "]\n",
    "\n",
    "query = \"What's an LLMChain?\"\n",
    "\n",
    "retriever = KNNRetriever.from_texts(documents, embeddings,k=2)\n",
    "\n",
    "# retrieve the most relevant documents\n",
    "result = retriever.invoke(query)\n",
    "top1_retrieved_doc = result[0].page_content  # return the top1 retrieved result\n",
    "\n",
    "print(top1_retrieved_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57bc58c-c630-4ea4-926d-2d247a47a08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What's an LLMChain?\",\n",
       " 'context': [Document(metadata={}, page_content='An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.'),\n",
       "  Document(metadata={}, page_content='A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.')],\n",
       " 'answer': 'An LLMChain is a chain that composes basic LLM functionality, consisting of a PromptTemplate and a language model. It formats the prompt using input and memory key values, and returns the LLM output. It combines these elements to facilitate language model interactions.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "#retriever = KNNRetriever.from_texts(documents, embeddings,k=2)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "query = \"What's an LLMChain?\"\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ae88e-b515-436c-9912-e1d92dcded15",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90b3044-37a4-48b6-b2c9-d644fc368e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -qU  rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf362f2c-d20b-4ba9-8f17-2efd66f27b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.'),\n",
       " Document(metadata={}, page_content='A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "retriever = BM25Retriever.from_texts(documents,k=2)\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2e2e36-c412-4d9c-89c4-e7759099707c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What's an LLMChain?\",\n",
       " 'context': [Document(metadata={}, page_content='An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.'),\n",
       "  Document(metadata={}, page_content='A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.')],\n",
       " 'answer': 'An LLMChain is a chain that composes basic LLM functionality, consisting of a PromptTemplate and a language model. It formats the prompt template using input key values and memory key values, then passes the formatted string to the LLM. It returns the LLM output.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "#retriever = KNNRetriever.from_texts(documents, embeddings,k=2)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "query = \"What's an LLMChain?\"\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5082f-e746-4d2b-88ca-490d52e9a5ee",
   "metadata": {},
   "source": [
    "##   ğŸ§  ä½œæ¥­ä¸‰ï¼šèªæ„ç†è§£ä»»å‹™ä¸­çš„ Embedding å¯¦é©—è¨­è¨ˆ\n",
    "\n",
    "**ç›®æ¨™ï¼š**\n",
    "\n",
    "* åˆ†æ embedding æ¨¡å‹åœ¨ä¸åŒèªå¢ƒä¸‹çš„è¡¨ç¾\n",
    "* æ¯”è¼ƒèªæ„ç›¸è¿‘ä½†å­—é¢ä¸åŒçš„ä¾‹å­\n",
    "* æ¯”è¼ƒä¸åŒ embedding æ¨¡å‹åœ¨ç›¸åŒä»»å‹™ä¸Šçš„è¡¨ç¾ (å¯é¸æ“‡)\n",
    "\n",
    "**å…§å®¹ï¼š**\n",
    "\n",
    "1.  **è¨­è¨ˆå¥å­å°ï¼š**\n",
    "    * è¨­è¨ˆåçµ„å¥å­å°ï¼Œæ¯çµ„åŒ…å«ä»¥ä¸‹è‡³å°‘å…©ç¨®èªæ„é—œä¿‚çš„çµ„åˆï¼š\n",
    "        * **åŒç¾©ï¼š** ã€Œä»–å¾ˆé–‹å¿ƒã€ vs ã€Œä»–æ„Ÿåˆ°æ„‰å¿«ã€\n",
    "        * **è¿‘ç¾©ï¼š** ã€Œé€™è¼›è»Šå¾ˆæ–°ã€ vs ã€Œé€™è¼›è»Šæ˜¯å…¨æ–°çš„ã€\n",
    "        * **åç¾©ï¼š** ã€Œä»–å¾ˆé«˜ã€ vs ã€Œä»–å¾ˆçŸ®ã€\n",
    "        * **ä¸Šä½/ä¸‹ä½è©ï¼š** ã€Œç‹—æ˜¯ä¸€ç¨®å‹•ç‰©ã€ vs ã€Œé»ƒé‡‘çµçŠ¬æ˜¯ä¸€ç¨®ç‹—ã€\n",
    "        * **å› æœé—œä¿‚ï¼š** ã€Œå› ç‚ºä¸‹é›¨ï¼Œæ‰€ä»¥è·¯å¾ˆæ¿•ã€ vs ã€Œè·¯å¾ˆæ¿•æ˜¯å› ç‚ºä¸‹é›¨ã€\n",
    "        * (é¼“å‹µåŠ å…¥è¤‡é›œå¥æˆ–å¤šç¾©è©)\n",
    "    * ç¢ºä¿æ¯çµ„å¥å­å°ä¸­ï¼Œè‡³å°‘æœ‰ä¸€å°æ˜¯èªæ„ç›¸è¿‘ï¼Œå¦ä¸€å°æ˜¯èªæ„ç›¸ç•°ã€‚\n",
    "\n",
    "2.  **è¨ˆç®—ç›¸ä¼¼åº¦ï¼š**\n",
    "    * é¸æ“‡ä¸€å€‹æˆ–å¤šå€‹ embedding æ¨¡å‹ (ä¾‹å¦‚ï¼š notebook ä¸­æåˆ°çš„æˆ–å…¶ä»–ä½ æ„Ÿèˆˆè¶£çš„æ¨¡å‹)ã€‚\n",
    "    * ä½¿ç”¨é¸æ“‡çš„ embedding æ¨¡å‹è¨ˆç®—æ¯çµ„å¥å­å°ä¹‹é–“çš„ç›¸ä¼¼åº¦ã€‚\n",
    "    * å»ºè­°ä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity) ä½œç‚ºè¨ˆç®—æ–¹æ³•ã€‚\n",
    "\n",
    "3.  **åˆ†æèˆ‡è©•ä¼°ï¼š**\n",
    "    * åˆ†ææ¨¡å‹çš„èªæ„ç†è§£èƒ½åŠ›ï¼š\n",
    "        * æ¨¡å‹æ˜¯å¦èƒ½æœ‰æ•ˆåˆ†è¾¨èªæ„ç›¸è¿‘èˆ‡èªæ„ç›¸ç•°çš„å¥å­å°ï¼Ÿ\n",
    "        * æ¨¡å‹å°æ–¼ä¸åŒèªæ„é—œä¿‚çš„è™•ç†èƒ½åŠ›æ˜¯å¦æœ‰å·®ç•°ï¼Ÿ\n",
    "        * æ¨¡å‹åœ¨è™•ç†è¤‡é›œå¥æˆ–å¤šç¾©è©æ™‚çš„è¡¨ç¾å¦‚ä½•ï¼Ÿ\n",
    "    * ä½¿ç”¨ä»¥ä¸‹æŒ‡æ¨™è©•ä¼°æ¨¡å‹è¡¨ç¾ï¼š\n",
    "        * **æº–ç¢ºåº¦ï¼š** æ¨¡å‹æ˜¯å¦èƒ½æ­£ç¢ºå€åˆ†èªæ„ç›¸è¿‘å’Œèªæ„ç›¸ç•°çš„å¥å­å°ï¼Ÿ (å¯è‡ªè¡Œå®šç¾©åˆ¤æ–·æ¨™æº–)\n",
    "        * **æ’åºèƒ½åŠ›ï¼š** å¦‚æœå°‡æ‰€æœ‰å¥å­å°ä¾ç›¸ä¼¼åº¦æ’åºï¼Œèªæ„æ›´ç›¸è¿‘çš„å¥å­å°æ˜¯å¦æ’åœ¨æ›´å‰é¢ï¼Ÿ\n",
    "    * (å¦‚æœé¸æ“‡å¤šå€‹ embedding æ¨¡å‹) æ¯”è¼ƒä¸åŒæ¨¡å‹ä¹‹é–“çš„å·®ç•°ã€‚\n",
    "\n",
    "**è©•åˆ†æ¨™æº–ï¼š**\n",
    "\n",
    "* å¥å­å°è¨­è¨ˆçš„å®Œæ•´æ€§èˆ‡å¤šæ¨£æ€§ (40%)\n",
    "* ç›¸ä¼¼åº¦è¨ˆç®—çš„æ­£ç¢ºæ€§ (30%)\n",
    "* åˆ†æèˆ‡è©•ä¼°çš„æ·±åº¦ (30%)\n",
    "* å ±å‘Šçš„æ¸…æ™°åº¦èˆ‡å®Œæ•´æ€§ (20%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0442c-774a-4cb6-93fa-f8bff74e0a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
