{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5f93a5-fa7c-4094-b4e9-76050ebb7b55",
   "metadata": {},
   "source": [
    "# Rerank\n",
    "本筆記本介紹如何開始使用Chroma向量儲存。\n",
    "- https://python.langchain.com/docs/integrations/document_transformers/rankllm-reranker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180e725-96bf-4dc8-941b-06cdaa90f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝套件\n",
    "!uv pip install -qU \"langchain-chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e607eee-2a4e-48d3-b8a8-03608b06a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99797401-6585-45ee-82ff-67d83a1119cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm模型設定\n",
    "# https://build.nvidia.com/deepseek-ai/deepseek-r1\n",
    "# nvapi-xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "  os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"meta/llama-4-maverick-17b-128e-instruct\", model_provider=\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476fdb9-bec9-4ed9-8e60-9fa4fed3275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding and rerank 模型\n",
    "# https://jina.ai/\n",
    "# jina_xxx\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"JINA_API_KEY\"):\n",
    "  os.environ[\"JINA_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n",
    "\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "embeddings = JinaEmbeddings(\n",
    "    jina_api_key=os.environ[\"JINA_API_KEY\"], model_name=\"jina-embeddings-v3\"\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "\n",
    "rerank = JinaRerank(model=\"jina-reranker-v2-base-multilingual\")  # 或使用 \"rerank-1\" 精準但較慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e583f-d169-4f24-a2cf-41a3dc82dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    #collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db30\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4d34a-33ff-418e-80b7-366af5012bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸出所有儲存的 document（包括 id, page_content, metadata）\n",
    "all_data = vector_store._collection.get()\n",
    "\n",
    "# 印出每筆紀錄的內容\n",
    "for i in range(len(all_data[\"ids\"])):\n",
    "    print(f\"ID: {all_data['ids'][i]}\")\n",
    "    print(f\"Document: {all_data['documents'][i]}\")\n",
    "    print(f\"Metadata: {all_data['metadatas'][i]}\")\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fde18-2499-4de1-9277-515ae16d10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by turning into retriever\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    ")\n",
    "retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cacb01-5fec-4c5f-a49c-a1ad2cba1698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 把向量資料庫轉換為 retriever，並指定檢索參數\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "query = \"RStealing from the bank is a crime\"\n",
    "docs = retriever.invoke(query)\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b1094-92c4-4d72-935c-33debe1e6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Stealing from the bank is a crime\"\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "rerank_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=rerank, base_retriever=retriever\n",
    ")\n",
    "\n",
    "rerank_docs = rerank_retriever.invoke(query)\n",
    "#print(rerank_docs)\n",
    "pretty_print_docs(rerank_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e803705-14f3-4d6c-af7c-f380297db3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(rerank_retriever, question_answer_chain)\n",
    "query = \"Stealing from the bank is a crime\"\n",
    "chain.invoke({\"input\": query}, filter={\"source\": \"news\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b5ee4-e11f-4faf-b746-196189cbb46b",
   "metadata": {},
   "source": [
    "##   作業\n",
    "\n",
    "**使用 Rerank 提升問答系統**\n",
    "\n",
    "Notebook 中創建了一個基本的問答系統，使用 Chroma 檢索相關文檔並使用 LLM 回答問題。\n",
    "\n",
    "**作業：**\n",
    "\n",
    "1.  **擴展問答系統：**\n",
    "    * 在 notebook 的基礎上，加入 Rerank 功能，提升系統回答問題的準確性。\n",
    "2.  **選擇文檔來源：**\n",
    "    * 選擇一個適合問答系統的文檔來源，例如：\n",
    "        * 網頁文章\n",
    "        * 產品說明書\n",
    "        * 法律文件\n",
    "        * 學術論文\n",
    "    * 請在報告中說明你選擇的文檔來源及其適用情境。\n",
    "3.  **實作 Rerank：**\n",
    "    * 實作 Rerank 功能，對向量資料庫檢索到的文檔進行重新排序。\n",
    "    * 可以選擇以下 Rerank 方法：\n",
    "        * 自訂 Rerank 演算法\n",
    "    * 請在報告中說明你選擇的 Rerank 方法及其原因。\n",
    "4.  **回答使用者問題：**\n",
    "    * 系統能夠根據 Rerank 後的文檔，回答使用者提出的問題。\n",
    "\n",
    "**評估標準：**\n",
    "\n",
    "* 系統是否能夠正確回答使用者提出的問題？ (40%)\n",
    "* Rerank 是否能夠提升檢索結果的相關性？ (30%)\n",
    "* Rerank 對系統效率的影響？ (30%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb80ee-64c7-404e-9493-005f8aa10aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
