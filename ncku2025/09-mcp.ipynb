{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ed033-b6fa-44f4-a6ac-f8baf0b1b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install langchain-mcp-adapters\n",
    "!uv pip install langchain-mcp-adapters langgraph langchain-openai langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9faa5feb-2265-4009-b919-60204b53eb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for NVIDIA:  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\uv\\langchaing\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:212: UserWarning: Found meta/llama-4-maverick-17b-128e-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LLM 模型\n",
    "# https://build.nvidia.com/deepseek-ai/deepseek-r1\n",
    "# nvapi-xxx\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "  os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"meta/llama-4-maverick-17b-128e-instruct\", model_provider=\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d5c9f-0893-4e12-9328-fad3f67fa3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# math_server.py\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Math\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# 適用於 Jupyter Notebook 或 async 環境\n",
    "#mcp.run(transport=\"stdio\")\n",
    "#await mcp.run_stdio_async()\n",
    "# 在 Notebook 或非終端機環境中運行\n",
    "await mcp.run_sse_async()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313fa13-aa65-4a8b-85e2-391202990265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [16224]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# weather_server.py\n",
    "from typing import List\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Math\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "    \n",
    "mcp = FastMCP(\"Weather\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather for location.\"\"\"\n",
    "    return \"It's always sunny in New York\"\n",
    "\n",
    "\n",
    "#mcp.run(transport=\"sse\")\n",
    "await mcp.run_sse_async()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd87b15-85ee-4937-bbf8-361d938f1ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
