{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c7ba09-bd42-4147-b1a5-9b91a1cf2be9",
   "metadata": {},
   "source": [
    "# Retrievers\n",
    "- https://python.langchain.com/docs/integrations/retrievers/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1d00b-d7e6-47ca-8994-4492acb386ae",
   "metadata": {},
   "source": [
    "## ArxivRetriever\n",
    "- https://python.langchain.com/docs/integrations/retrievers/arxiv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85e05db-0e04-4efa-ae66-105f34446d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -qU langchain-community arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aea8c5b-16bc-4b7b-ab3b-e3751a48d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "retriever = ArxivRetriever(\n",
    "    load_max_docs=2,\n",
    "    get_ful_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6492380-3301-4a6f-b934-93044d737814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Entry ID': 'http://arxiv.org/abs/1605.08386v1',\n",
       " 'Published': datetime.date(2016, 5, 26),\n",
       " 'Title': 'Heat-bath random walks with Markov bases',\n",
       " 'Authors': 'Caprice Stanley, Tobias Windisch'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"1605.08386\")\n",
    "docs[0].metadata  # meta-information of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626fa7ec-7332-41a1-88f5-fe8f793a5802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a ge'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:400]  # a content of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851c5c54-d7e0-4b1c-a3e2-3313aa4955be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Entry ID': 'http://arxiv.org/abs/2309.03905v2',\n",
       " 'Published': datetime.date(2023, 9, 11),\n",
       " 'Title': 'ImageBind-LLM: Multi-modality Instruction Tuning',\n",
       " 'Authors': 'Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is the ImageBind model?\")\n",
    "docs[1].metadata  # meta-information of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "493b6139-ecd9-4f62-972f-6e2962ec9585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We present ImageBind-LLM, a multi-modality instruction tuning method of large\\nlanguage models (LLMs) via ImageBind. Existing works mainly focus on language\\nand image instruction tuning, different from which, our ImageBind-LLM can\\nrespond to multi-modality conditions, including audio, 3D point clouds, video,\\nand their embedding-space arithmetic by only image-text alignment training.\\nDuring training'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content[:400]  # a content of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614754c4-6682-47ce-b3eb-d9a53c2925e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Doc 1] Metadata:\n",
      "  Entry ID: http://arxiv.org/abs/2305.05665v2\n",
      "  Published: 2023-05-31\n",
      "  Title: ImageBind: One Embedding Space To Bind Them All\n",
      "  Authors: Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra\n",
      "\n",
      "[Doc 2] Metadata:\n",
      "  Entry ID: http://arxiv.org/abs/2309.03905v2\n",
      "  Published: 2023-09-11\n",
      "  Title: ImageBind-LLM: Multi-modality Instruction Tuning\n",
      "  Authors: Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao\n",
      "\n",
      "[Doc 3] Metadata:\n",
      "  Entry ID: http://arxiv.org/abs/2503.03648v1\n",
      "  Published: 2025-03-05\n",
      "  Title: Modelowanie nieliniowej charakterystyki szerokopasmowych wzmacniaczy radiowych o zmiennym napięciu zasilania; Modeling Nonlinear Characteristics of Wideband Radio Frequency Amplifiers with Variable Supply Voltage\n",
      "  Authors: Kornelia Kostrzewska, Paweł Kryszkiewicz\n"
     ]
    }
   ],
   "source": [
    "# 顯示每篇文件的 metadata\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n[Doc {i+1}] Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71603cc0-71cc-40b6-b8ad-72373f0f6cbd",
   "metadata": {},
   "source": [
    "# chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3896971b-5e30-4c1a-8c0f-987fd1f1013c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for NVIDIA:  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\uv\\langchaing\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:212: UserWarning: Found meta/llama-4-maverick-17b-128e-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# chain\n",
    "# https://build.nvidia.com/deepseek-ai/deepseek-r1\n",
    "# nvapi-xxx\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "  os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"meta/llama-4-maverick-17b-128e-instruct\", model_provider=\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5e8706-e0f6-4909-a6a9-f88bc5227814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the ImageBind model?',\n",
       " 'context': [Document(metadata={'Entry ID': 'http://arxiv.org/abs/2305.05665v2', 'Published': datetime.date(2023, 5, 31), 'Title': 'ImageBind: One Embedding Space To Bind Them All', 'Authors': 'Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra'}, page_content=\"We present ImageBind, an approach to learn a joint embedding across six\\ndifferent modalities - images, text, audio, depth, thermal, and IMU data. We\\nshow that all combinations of paired data are not necessary to train such a\\njoint embedding, and only image-paired data is sufficient to bind the\\nmodalities together. ImageBind can leverage recent large scale vision-language\\nmodels, and extends their zero-shot capabilities to new modalities just by\\nusing their natural pairing with images. It enables novel emergent applications\\n'out-of-the-box' including cross-modal retrieval, composing modalities with\\narithmetic, cross-modal detection and generation. The emergent capabilities\\nimprove with the strength of the image encoder and we set a new\\nstate-of-the-art on emergent zero-shot recognition tasks across modalities,\\noutperforming specialist supervised models. Finally, we show strong few-shot\\nrecognition results outperforming prior work, and that ImageBind serves as a\\nnew way to evaluate vision models for visual and non-visual tasks.\"),\n",
       "  Document(metadata={'Entry ID': 'http://arxiv.org/abs/2309.03905v2', 'Published': datetime.date(2023, 9, 11), 'Title': 'ImageBind-LLM: Multi-modality Instruction Tuning', 'Authors': 'Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao'}, page_content=\"We present ImageBind-LLM, a multi-modality instruction tuning method of large\\nlanguage models (LLMs) via ImageBind. Existing works mainly focus on language\\nand image instruction tuning, different from which, our ImageBind-LLM can\\nrespond to multi-modality conditions, including audio, 3D point clouds, video,\\nand their embedding-space arithmetic by only image-text alignment training.\\nDuring training, we adopt a learnable bind network to align the embedding space\\nbetween LLaMA and ImageBind's image encoder. Then, the image features\\ntransformed by the bind network are added to word tokens of all layers in\\nLLaMA, which progressively injects visual instructions via an attention-free\\nand zero-initialized gating mechanism. Aided by the joint embedding of\\nImageBind, the simple image-text training enables our model to exhibit superior\\nmulti-modality instruction-following capabilities. During inference, the\\nmulti-modality inputs are fed into the corresponding ImageBind encoders, and\\nprocessed by a proposed visual cache model for further cross-modal embedding\\nenhancement. The training-free cache model retrieves from three million image\\nfeatures extracted by ImageBind, which effectively mitigates the\\ntraining-inference modality discrepancy. Notably, with our approach,\\nImageBind-LLM can respond to instructions of diverse modalities and demonstrate\\nsignificant language generation quality. Code is released at\\nhttps://github.com/OpenGVLab/LLaMA-Adapter.\"),\n",
       "  Document(metadata={'Entry ID': 'http://arxiv.org/abs/2503.03648v1', 'Published': datetime.date(2025, 3, 5), 'Title': 'Modelowanie nieliniowej charakterystyki szerokopasmowych wzmacniaczy radiowych o zmiennym napięciu zasilania; Modeling Nonlinear Characteristics of Wideband Radio Frequency Amplifiers with Variable Supply Voltage', 'Authors': 'Kornelia Kostrzewska, Paweł Kryszkiewicz'}, page_content='The work aims to propose a new nonlinear characteristics model for a wideband\\nradio amplifier of variable supply voltage. An extended Rapp model proposal is\\npresented. The proposed model has been verified by measurements of three\\ndifferent amplifiers. This model can be used to design frontend-aware 6G\\nsystems.\\n  --\\n  Praca ma na celu zaproponowanie nowego modelu dla nieliniowej charakterystyki\\nwzmacniacza radiowego ze zmiennym napi\\\\k{e}ciem zasilania pracuj\\\\k{a}cym w\\nszerokim zakresie cz\\\\k{e}stotliwo\\\\\\'sci. Przedstawiona zosta{\\\\l}a propozycja\\nrozszerzonego modelu Rappa. Zaproponowany model zweryfikowano na podstawie\\npomiar\\\\\\'ow charakterystyk trzech r\\\\\\'o\\\\.znych wzmacniaczy. Model ten mo\\\\.ze\\nby\\\\\\'c wykorzystany do projektowania system\\\\\\'ow 6G \"\\\\\\'swiadomych\"\\nniedoskona{\\\\l}o\\\\\\'sci uk{\\\\l}ad\\\\\\'ow wej\\\\\\'sciowo-wyj\\\\\\'sciowych.')],\n",
       " 'answer': 'The ImageBind model is an approach to learn a joint embedding across six different modalities: images, text, audio, depth, thermal, and IMU data. It uses image-paired data to bind the modalities together and can leverage large-scale vision-language models. ImageBind enables novel emergent applications such as cross-modal retrieval and generation.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "\n",
    "retriever = ArxivRetriever(load_max_docs=2, get_ful_documents=True)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "query = \"What is the ImageBind model?\"\n",
    "chain.invoke({\"input\": query})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d311bb-af86-40e0-a33c-e5863e489804",
   "metadata": {},
   "source": [
    "##   作業\n",
    "\n",
    "**擴展檢索鏈 (Retrieval Chain)**\n",
    "\n",
    "Notebook 中創建了一個基本的檢索鏈，將 ArxivRetriever 和 LLM 連接起來回答問題。\n",
    "\n",
    "1.  **基本功能：**\n",
    "    * 在 notebook 的基礎上，擴展檢索鏈的功能，使其更加實用。\n",
    "\n",
    "2.  **擴展方向（選擇至少兩項）：**\n",
    "    * **文件篩選：** 讓使用者可以根據作者、日期、關鍵字等條件篩選文獻。\n",
    "    * **摘要生成：** 自動總結檢索到的文獻內容。\n",
    "    * **多源檢索：** 整合多個 Retriever，從不同來源檢索資訊 (例如 Arxiv、Web、自訂資料庫)。\n",
    "    * **結果排序：** 根據相關性對檢索結果進行排序和評分。\n",
    "    * **複雜查詢處理：** 處理包含多個關鍵字和條件的查詢。\n",
    "\n",
    "**評估標準：**\n",
    "\n",
    "* 檢索鏈是否能夠正確檢索並呈現相關文獻？ (50%)\n",
    "* 擴展功能是否能夠有效提升檢索的實用性？ (50%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe8ec6-0f65-4261-9c11-a470a9156101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
